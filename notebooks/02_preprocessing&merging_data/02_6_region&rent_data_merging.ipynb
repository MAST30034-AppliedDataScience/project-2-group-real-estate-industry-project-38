{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set up file paths and directories\n",
    "base_dir = '../..'\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "raw_rent_dir = os.path.join(data_dir, 'raw', 'rent_data') \n",
    "os.makedirs(raw_rent_dir, exist_ok=True)\n",
    "landing_dir = os.path.join(data_dir, 'landing')\n",
    "raw_other_dir = os.path.join(data_dir, 'raw', 'other_data')\n",
    "raw_dir = os.path.join(data_dir, 'raw')\n",
    "curated_dir = os.path.join(data_dir, 'curated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Suburb                  SA2  Jun 1999 count  Jun 1999 median  \\\n",
      "0  Colac-Otway                Colac            99.0            115.0   \n",
      "1  Colac-Otway      Colac Surrounds            99.0            115.0   \n",
      "2  Colac-Otway                Otway            99.0            115.0   \n",
      "3  Corangamite           Camperdown            36.0             93.0   \n",
      "4  Corangamite  Corangamite - North            36.0             93.0   \n",
      "\n",
      "   Sep 1999 count  Sep 1999 median  Dec 1999 count  Dec 1999 median  \\\n",
      "0            80.0            113.0            82.0            115.0   \n",
      "1            80.0            113.0            82.0            115.0   \n",
      "2            80.0            113.0            82.0            115.0   \n",
      "3            44.0             90.0            47.0            100.0   \n",
      "4            44.0             90.0            47.0            100.0   \n",
      "\n",
      "   Mar 2000 count  Mar 2000 median  ...  Mar 2023 count  Mar 2023 median  \\\n",
      "0            96.0            115.0  ...            86.0            423.0   \n",
      "1            96.0            115.0  ...            86.0            423.0   \n",
      "2            96.0            115.0  ...            86.0            423.0   \n",
      "3            52.0            100.0  ...            41.0            380.0   \n",
      "4            52.0            100.0  ...            41.0            380.0   \n",
      "\n",
      "   Jun 2023 count  Jun 2023 median  Sep 2023 count  Sep 2023 median  \\\n",
      "0           109.0            410.0           108.0            435.0   \n",
      "1           109.0            410.0           108.0            435.0   \n",
      "2           109.0            410.0           108.0            435.0   \n",
      "3            45.0            350.0            47.0            400.0   \n",
      "4            45.0            350.0            47.0            400.0   \n",
      "\n",
      "   Dec 2023 count  Dec 2023 median  Mar 2024 count  Mar 2024 median  \n",
      "0            87.0            410.0            92.0            420.0  \n",
      "1            87.0            410.0            92.0            420.0  \n",
      "2            87.0            410.0            92.0            420.0  \n",
      "3            36.0            363.0            35.0            360.0  \n",
      "4            36.0            363.0            35.0            360.0  \n",
      "\n",
      "[5 rows x 202 columns]\n",
      "Final rent data saved to ../../data/raw/rent_data/final_rent_data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load the downloaded rental data Excel file from raw folder\n",
    "rental_data_path = os.path.join(landing_dir, 'other_data', 'Quarterly_median_rent_March_2024.xlsx')\n",
    "\n",
    "# Step 2: Load the 'All properties' sheet from the Excel file, skipping the first two rows, and drop NaN columns\n",
    "df_cleaned = pd.read_excel(rental_data_path, sheet_name='All Properties', header=2).dropna(axis=1, how='all')\n",
    "\n",
    "# Step 3: Forward-fill the 'Region' and 'Suburb' columns to ensure no missing values\n",
    "df_cleaned['Unnamed: 0'] = df_cleaned['Unnamed: 0'].ffill()  # Region\n",
    "df_cleaned['Unnamed: 1'] = df_cleaned['Unnamed: 1'].ffill()  # Suburb\n",
    "\n",
    "# Step 4: Manually read the dates from the second row (assuming the date is in the even columns starting from column 2)\n",
    "date_headers = pd.read_excel(rental_data_path, sheet_name='All Properties', header=None).iloc[1, 2::2]\n",
    "\n",
    "# Step 5: Create an empty DataFrame to store transformed data\n",
    "transformed_data = pd.DataFrame()\n",
    "\n",
    "# Step 6: Iterate through rows to combine dates with count and median data\n",
    "for i, row in df_cleaned.iterrows():\n",
    "    region = row['Unnamed: 0']  # Region\n",
    "    suburb = row['Unnamed: 1']  # Suburb\n",
    "    row_data = {'Region': region, 'Suburb': suburb}\n",
    "\n",
    "    # Loop over the columns, skipping by 2 (Count and Median pairs)\n",
    "    for j in range(2, len(row), 2):\n",
    "        date = date_headers.iloc[(j - 2) // 2]  # Extract the date corresponding to this column\n",
    "        count = row[j]  # Count value\n",
    "        median = row[j + 1]  # Median value\n",
    "\n",
    "        # Create column names for each date's count and median\n",
    "        count_col = f'{date} count'\n",
    "        median_col = f'{date} median'\n",
    "\n",
    "        # Add the count and median values to the row_data dictionary\n",
    "        row_data[count_col] = count\n",
    "        row_data[median_col] = median\n",
    "\n",
    "    # Append the row data to the transformed_data DataFrame\n",
    "    transformed_data = pd.concat([transformed_data, pd.DataFrame([row_data])], ignore_index=True)\n",
    "# Step 7: Clean and convert rent data\n",
    "def clean_rent_data(df):\n",
    "    for col in df.columns:\n",
    "        if 'count' in col or 'median' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert non-numeric values to NaN\n",
    "    return df\n",
    "\n",
    "# Step 8: Apply cleaning and preprocess the transformed rental data\n",
    "cleaned_rent_data = clean_rent_data(transformed_data)\n",
    "\n",
    "# Step 9: Drop rows where 'Region' or 'Suburb' is missing and ensure no NaN values in important columns\n",
    "cleaned_rent_data = cleaned_rent_data.dropna(subset=['Region', 'Suburb'])\n",
    "\n",
    "# Drop rows where any 'count' or 'median' column contains NaN values\n",
    "count_columns = [col for col in cleaned_rent_data.columns if 'count' in col]\n",
    "median_columns = [col for col in cleaned_rent_data.columns if 'median' in col]\n",
    "cleaned_rent_data = cleaned_rent_data.dropna(subset=count_columns + median_columns)\n",
    "\n",
    "# Step 10: Load the LGA to SA2 correspondence data from raw folder\n",
    "lga_sa2_file_path = os.path.join(landing_dir, 'other_data', 'CG_SA2_2021_LGA_2022.csv')\n",
    "correspondence_data = pd.read_csv(lga_sa2_file_path)[['SA2_NAME_2021', 'LGA_NAME_2022']]\n",
    "\n",
    "# Step 11: Standardize the 'Suburb' and 'LGA_NAME_2022' columns for merging\n",
    "# Remove spaces, hyphens, and convert to lowercase for consistent merging\n",
    "cleaned_rent_data['Suburb_standardized'] = cleaned_rent_data['Suburb'].str.replace('-', '').str.replace(' ', '').str.lower()\n",
    "correspondence_data['LGA_standardized'] = correspondence_data['LGA_NAME_2022'].str.replace('-', '').str.replace(' ', '').str.lower()\n",
    "\n",
    "# Step 12: Merge the cleaned rent data with the SA2-LGA correspondence data\n",
    "merged_rent_data = pd.merge(cleaned_rent_data, correspondence_data, left_on='Suburb_standardized', right_on='LGA_standardized', how='left')\n",
    "\n",
    "# Step 13: Clean up and remove unnecessary columns after the merge\n",
    "merged_rent_data = merged_rent_data.drop(columns=['Suburb_standardized', 'LGA_standardized', 'LGA_NAME_2022', 'Region'])\n",
    "\n",
    "# Rename the 'SA2_NAME_2021' column to 'SA2' for consistency\n",
    "merged_rent_data.rename(columns={'SA2_NAME_2021': 'SA2'}, inplace=True)\n",
    "\n",
    "# Step 14: Reorganize the columns so that 'SA2' is in the second position\n",
    "cols = list(merged_rent_data.columns)\n",
    "cols.insert(1, cols.pop(cols.index('SA2')))  # Move 'SA2' to the second position\n",
    "merged_rent_data = merged_rent_data[cols]\n",
    "\n",
    "# Step 15: Save the final processed rent data to 'curated_01'\n",
    "output_rent_path = os.path.join(raw_rent_dir, 'final_rent_data.csv')\n",
    "merged_rent_data.to_csv(output_rent_path, index=False)\n",
    "\n",
    "# Display the first few rows of the cleaned and merged rent data\n",
    "print(merged_rent_data.head())\n",
    "print(f\"Final rent data saved to {output_rent_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criminal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/gylgnm051zv9rk4m5lzq6c880000gn/T/ipykernel_17242/3529672311.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  criminal_data_postcode['Median Criminal Count'] = criminal_data_postcode_median\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             SA2  Median Criminal Count\n",
      "0     Abbotsford              52.000000\n",
      "1   Airport West              85.000000\n",
      "2    Albert Park              85.500000\n",
      "3  Albury - East             121.000000\n",
      "4      Alexandra              22.555556\n",
      "Aggregated criminal data saved to ../../data/raw/rent_data/aggregated_criminal_data_sa2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# Step 1: Load the pre-downloaded criminal data from the raw folder\n",
    "criminal_data_path = os.path.join(raw_other_dir, 'criminal_data.csv')\n",
    "criminal_data = pd.read_csv(criminal_data_path)\n",
    "\n",
    "# Step 2: Create a new column combining 'Year' and 'Year ending' for pivoting\n",
    "criminal_data['Year_Month'] = criminal_data['Year'].astype(str) + ' ' + criminal_data['Year ending']\n",
    "\n",
    "# Step 3: Pivot the criminal data by 'Postcode' and 'Year_Month' to create a table of counts\n",
    "criminal_pivot = pd.pivot_table(criminal_data, \n",
    "                                index=['Postcode'], \n",
    "                                columns='Year_Month', \n",
    "                                aggfunc='size', \n",
    "                                fill_value=0).reset_index()\n",
    "\n",
    "# Step 4: Calculate the median criminal count for each Postcode across all years\n",
    "criminal_data_postcode_median = criminal_pivot.median(axis=1)\n",
    "criminal_data_postcode = criminal_pivot[['Postcode']]\n",
    "criminal_data_postcode['Median Criminal Count'] = criminal_data_postcode_median\n",
    "\n",
    "# Step 5: Load the POSTCODE to SA2 mapping data from raw folder\n",
    "postcode_sa2_file_path = os.path.join(landing_dir, 'other_data', 'CG_POSTCODE_2021_SA2_2021.xlsx')\n",
    "postcode_sa2_data = pd.read_excel(postcode_sa2_file_path, usecols=['POSTCODE', 'SA2_NAME_2021'])\n",
    "\n",
    "# Step 6: Merge the criminal data with the POSTCODE to SA2 correspondence file\n",
    "merged_criminal_data = pd.merge(criminal_data_postcode, postcode_sa2_data, left_on='Postcode', right_on='POSTCODE', how='left')\n",
    "\n",
    "# Step 7: Drop unnecessary columns after merging\n",
    "merged_criminal_data = merged_criminal_data[['SA2_NAME_2021', 'Median Criminal Count']]\n",
    "\n",
    "# Step 8: Aggregate the merged criminal data by SA2\n",
    "aggregated_criminal_data = merged_criminal_data.groupby('SA2_NAME_2021').agg({\n",
    "    'Median Criminal Count': 'mean'  # Aggregating the median count by SA2\n",
    "}).reset_index()\n",
    "\n",
    "# Step 9: Rename 'SA2_NAME_2021' to 'SA2' for consistency across datasets\n",
    "aggregated_criminal_data.rename(columns={'SA2_NAME_2021': 'SA2'}, inplace=True)\n",
    "\n",
    "# Step 10: Save the final aggregated criminal data to 'curated_01'\n",
    "aggregated_criminal_data_file = os.path.join(raw_rent_dir, 'aggregated_criminal_data_sa2.csv')\n",
    "aggregated_criminal_data.to_csv(aggregated_criminal_data_file, index=False)\n",
    "\n",
    "# Display the first few rows of the aggregated criminal data\n",
    "print(aggregated_criminal_data.head())\n",
    "print(f\"Aggregated criminal data saved to {aggregated_criminal_data_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Region Data by SA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/gylgnm051zv9rk4m5lzq6c880000gn/T/ipykernel_17242/2323979638.py:45: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  sa2_data['centroid'] = sa2_data.geometry.centroid\n",
      "/Users/xxu/anaconda3/lib/python3.11/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    SA2  entertainments_count  hospital_count  park_count  \\\n",
      "0  Melbourne CBD - East                 317.0             NaN         2.0   \n",
      "1  Melbourne CBD - West                 153.0             NaN         3.0   \n",
      "2               Fitzroy                 135.0             2.0        13.0   \n",
      "3               Carlton                  97.0             1.0        21.0   \n",
      "4               Geelong                  94.0             3.0        27.0   \n",
      "\n",
      "   psf_count  school2_count  school1_count  shop_count  stop_count  \\\n",
      "0        3.0            7.0            2.0        10.0        32.0   \n",
      "1        3.0            4.0            2.0         5.0        67.0   \n",
      "2        1.0            6.0            3.0        14.0        35.0   \n",
      "3        1.0           10.0            2.0         9.0        52.0   \n",
      "4        2.0           11.0            7.0         4.0       122.0   \n",
      "\n",
      "   population  ...  nUnEmployed_2011  nUnEmployed_2016  nUnEmployed_2021  \\\n",
      "0 -209.314286  ...             400.0             679.0             521.0   \n",
      "1   77.457143  ...             395.0            1035.0             927.0   \n",
      "2  -51.971429  ...             329.0             428.0             292.0   \n",
      "3 -285.485714  ...             912.0            1564.0            1141.0   \n",
      "4   53.171429  ...             376.0             383.0             343.0   \n",
      "\n",
      "   nRented_2011  nRented_2016  nRented_2021  nHomeless_2011  nHomeless_2016  \\\n",
      "0        1948.0        2896.0        3276.0            58.0           413.0   \n",
      "1        2106.0        3962.0        6125.0            53.0           266.0   \n",
      "2        2275.0        2503.0        2685.0           319.0           223.0   \n",
      "3        4787.0        6289.0        5133.0           197.0           129.0   \n",
      "4        1854.0        1986.0        2340.0            62.0            85.0   \n",
      "\n",
      "   nHomeless_2021  distance_to_cbd  \n",
      "0           141.0         0.555785  \n",
      "1           201.0         0.576042  \n",
      "2           159.0         1.947158  \n",
      "3           110.0         1.406929  \n",
      "4            76.0        65.037556  \n",
      "\n",
      "[5 rows x 93 columns]\n",
      "Final region data saved to ../../data/raw/rent_data/final_region_data_with_distance.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "# Load the facility and region growth rates data from CSV files\n",
    "facility_file_path = os.path.join(raw_dir, 'facility_count', 'facility_merged.csv')\n",
    "region_growth_rates_file_path = os.path.join(raw_dir, 'region_data', 'region_growth_rates.csv')\n",
    "range_region_data_file_path = os.path.join(raw_dir, 'region_data', 'range_region_data.csv')\n",
    "\n",
    "facility_data = pd.read_csv(facility_file_path)\n",
    "region_growth_rates_data = pd.read_csv(region_growth_rates_file_path)\n",
    "range_region_data = pd.read_csv(range_region_data_file_path)\n",
    "\n",
    "# Step 1: Standardize region names for consistency across datasets\n",
    "facility_data['region_standardized'] = facility_data['region'].str.replace(' ', '')\n",
    "region_growth_rates_data['region_standardized'] = region_growth_rates_data['Region'].str.replace(' ', '')\n",
    "range_region_data['region_standardized'] = range_region_data['Region'].str.replace(' ', '')\n",
    "\n",
    "# Step 2: Merge facility data and region growth rates using standardized region names\n",
    "merged_data = pd.merge(facility_data, region_growth_rates_data, on='region_standardized', how='left')\n",
    "\n",
    "# Merge the result with range region data on the standardized region names\n",
    "merged_data = pd.merge(merged_data, range_region_data, on='region_standardized', how='left')\n",
    "\n",
    "# Step 3: Drop rows where the population is NaN to ensure regions are only from Victoria\n",
    "merged_data = merged_data.dropna(subset=['population'])\n",
    "\n",
    "# Step 4: Drop unnecessary columns after the merge (e.g., 'region_standardized', 'Region_x', 'Region_y')\n",
    "merged_data = merged_data.drop(columns=['region_standardized', 'Region_x', 'Region_y'])\n",
    "\n",
    "# Step 5: Rename 'region' column to 'SA2' if necessary\n",
    "if 'region' in merged_data.columns:\n",
    "    merged_data.rename(columns={'region': 'SA2'}, inplace=True)\n",
    "\n",
    "# Step 6: Load and process the SA2 shapefile to calculate the distance to Melbourne CBD\n",
    "shapefile_path = os.path.join(landing_dir, 'region_data', 'sa2_dataset', 'sa2_unzip', 'SA2_2021_AUST_GDA2020.shp')\n",
    "sa2_data = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Define Melbourne CBD coordinates\n",
    "melbourne_cbd_coords = (-37.8124, 144.9623)\n",
    "\n",
    "# Calculate centroids for each SA2 region\n",
    "sa2_data['centroid'] = sa2_data.geometry.centroid\n",
    "sa2_data['centroid_lat'] = sa2_data['centroid'].y\n",
    "sa2_data['centroid_lon'] = sa2_data['centroid'].x\n",
    "\n",
    "# Filter out rows with missing centroid values\n",
    "sa2_data_valid = sa2_data.dropna(subset=['centroid_lat', 'centroid_lon'])\n",
    "\n",
    "# Function to calculate the distance to Melbourne CBD\n",
    "def calculate_distance_to_cbd(row):\n",
    "    centroid_coords = (row['centroid_lat'], row['centroid_lon'])\n",
    "    return geodesic(centroid_coords, melbourne_cbd_coords).km\n",
    "\n",
    "# Step 7: Apply the distance calculation to each SA2 region\n",
    "sa2_data_valid['distance_to_cbd'] = sa2_data_valid.apply(calculate_distance_to_cbd, axis=1)\n",
    "\n",
    "# Step 8: Merge the SA2 data with the aggregated region data\n",
    "merged_sa2_data = merged_data.merge(sa2_data_valid[['SA2_NAME21', 'distance_to_cbd']], left_on='SA2', right_on='SA2_NAME21')\n",
    "\n",
    "# Step 9: Drop any unnecessary columns after the merge (e.g., 'SA2_NAME21')\n",
    "merged_sa2_data = merged_sa2_data.drop(columns=['SA2_NAME21'])\n",
    "\n",
    "# Step 10: Save the final processed region data with distance to Melbourne CBD\n",
    "final_output_path = os.path.join(raw_rent_dir, 'final_region_data_with_distance.csv')\n",
    "merged_sa2_data.to_csv(final_output_path, index=False)\n",
    "\n",
    "# Display the first few rows to verify the final result\n",
    "print(merged_sa2_data.head())\n",
    "print(f\"Final region data saved to {final_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criminal Data: \n",
      "             SA2  Median Criminal Count\n",
      "0     Abbotsford              52.000000\n",
      "1   Airport West              85.000000\n",
      "2    Albert Park              85.500000\n",
      "3  Albury - East             121.000000\n",
      "4      Alexandra              22.555556\n",
      "Rent Data: \n",
      "        Suburb                  SA2  Jun 1999 count  Jun 1999 median  \\\n",
      "0  Colac-Otway                Colac            99.0            115.0   \n",
      "1  Colac-Otway      Colac Surrounds            99.0            115.0   \n",
      "2  Colac-Otway                Otway            99.0            115.0   \n",
      "3  Corangamite           Camperdown            36.0             93.0   \n",
      "4  Corangamite  Corangamite - North            36.0             93.0   \n",
      "\n",
      "   Sep 1999 count  Sep 1999 median  Dec 1999 count  Dec 1999 median  \\\n",
      "0            80.0            113.0            82.0            115.0   \n",
      "1            80.0            113.0            82.0            115.0   \n",
      "2            80.0            113.0            82.0            115.0   \n",
      "3            44.0             90.0            47.0            100.0   \n",
      "4            44.0             90.0            47.0            100.0   \n",
      "\n",
      "   Mar 2000 count  Mar 2000 median  ...  Mar 2023 count  Mar 2023 median  \\\n",
      "0            96.0            115.0  ...            86.0            423.0   \n",
      "1            96.0            115.0  ...            86.0            423.0   \n",
      "2            96.0            115.0  ...            86.0            423.0   \n",
      "3            52.0            100.0  ...            41.0            380.0   \n",
      "4            52.0            100.0  ...            41.0            380.0   \n",
      "\n",
      "   Jun 2023 count  Jun 2023 median  Sep 2023 count  Sep 2023 median  \\\n",
      "0           109.0            410.0           108.0            435.0   \n",
      "1           109.0            410.0           108.0            435.0   \n",
      "2           109.0            410.0           108.0            435.0   \n",
      "3            45.0            350.0            47.0            400.0   \n",
      "4            45.0            350.0            47.0            400.0   \n",
      "\n",
      "   Dec 2023 count  Dec 2023 median  Mar 2024 count  Mar 2024 median  \n",
      "0            87.0            410.0            92.0            420.0  \n",
      "1            87.0            410.0            92.0            420.0  \n",
      "2            87.0            410.0            92.0            420.0  \n",
      "3            36.0            363.0            35.0            360.0  \n",
      "4            36.0            363.0            35.0            360.0  \n",
      "\n",
      "[5 rows x 202 columns]\n",
      "Region Data: \n",
      "                    SA2  entertainments_count  hospital_count  park_count  \\\n",
      "0  Melbourne CBD - East                 317.0             NaN         2.0   \n",
      "1  Melbourne CBD - West                 153.0             NaN         3.0   \n",
      "2               Fitzroy                 135.0             2.0        13.0   \n",
      "3               Carlton                  97.0             1.0        21.0   \n",
      "4               Geelong                  94.0             3.0        27.0   \n",
      "\n",
      "   psf_count  school2_count  school1_count  shop_count  stop_count  \\\n",
      "0        3.0            7.0            2.0        10.0        32.0   \n",
      "1        3.0            4.0            2.0         5.0        67.0   \n",
      "2        1.0            6.0            3.0        14.0        35.0   \n",
      "3        1.0           10.0            2.0         9.0        52.0   \n",
      "4        2.0           11.0            7.0         4.0       122.0   \n",
      "\n",
      "   population  ...  nUnEmployed_2011  nUnEmployed_2016  nUnEmployed_2021  \\\n",
      "0 -209.314286  ...             400.0             679.0             521.0   \n",
      "1   77.457143  ...             395.0            1035.0             927.0   \n",
      "2  -51.971429  ...             329.0             428.0             292.0   \n",
      "3 -285.485714  ...             912.0            1564.0            1141.0   \n",
      "4   53.171429  ...             376.0             383.0             343.0   \n",
      "\n",
      "   nRented_2011  nRented_2016  nRented_2021  nHomeless_2011  nHomeless_2016  \\\n",
      "0        1948.0        2896.0        3276.0            58.0           413.0   \n",
      "1        2106.0        3962.0        6125.0            53.0           266.0   \n",
      "2        2275.0        2503.0        2685.0           319.0           223.0   \n",
      "3        4787.0        6289.0        5133.0           197.0           129.0   \n",
      "4        1854.0        1986.0        2340.0            62.0            85.0   \n",
      "\n",
      "   nHomeless_2021  distance_to_cbd  \n",
      "0           141.0         0.555785  \n",
      "1           201.0         0.576042  \n",
      "2           159.0         1.947158  \n",
      "3           110.0         1.406929  \n",
      "4            76.0        65.037556  \n",
      "\n",
      "[5 rows x 93 columns]\n",
      "Final Merged Data: \n",
      "        Suburb                  SA2  Jun 1999 count  Jun 1999 median  \\\n",
      "0  Colac-Otway                Colac            99.0            115.0   \n",
      "1  Colac-Otway      Colac Surrounds            99.0            115.0   \n",
      "2  Colac-Otway                Otway            99.0            115.0   \n",
      "3  Corangamite           Camperdown            36.0             93.0   \n",
      "4  Corangamite  Corangamite - North            36.0             93.0   \n",
      "\n",
      "   Sep 1999 count  Sep 1999 median  Dec 1999 count  Dec 1999 median  \\\n",
      "0            80.0            113.0            82.0            115.0   \n",
      "1            80.0            113.0            82.0            115.0   \n",
      "2            80.0            113.0            82.0            115.0   \n",
      "3            44.0             90.0            47.0            100.0   \n",
      "4            44.0             90.0            47.0            100.0   \n",
      "\n",
      "   Mar 2000 count  Mar 2000 median  ...  nUnEmployed_2011  nUnEmployed_2016  \\\n",
      "0            96.0            115.0  ...             204.0             245.0   \n",
      "1            96.0            115.0  ...              77.0              82.0   \n",
      "2            96.0            115.0  ...              75.0              69.0   \n",
      "3            52.0            100.0  ...              52.0              67.0   \n",
      "4            52.0            100.0  ...              84.0              99.0   \n",
      "\n",
      "   nUnEmployed_2021  nRented_2011  nRented_2016  nRented_2021  nHomeless_2011  \\\n",
      "0             182.0        1127.0        1170.0        1323.0            44.0   \n",
      "1              55.0         239.0         243.0         258.0             4.0   \n",
      "2              73.0         271.0         292.0         313.0             3.0   \n",
      "3              39.0         308.0         308.0         294.0            16.0   \n",
      "4              88.0         365.0         347.0         379.0            14.0   \n",
      "\n",
      "   nHomeless_2016  nHomeless_2021  distance_to_cbd  \n",
      "0            53.0            62.0       135.702461  \n",
      "1             NaN             NaN       128.254358  \n",
      "2             3.0            27.0       153.910737  \n",
      "3            13.0            13.0       164.923626  \n",
      "4            11.0            11.0       151.438268  \n",
      "\n",
      "[5 rows x 295 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/gylgnm051zv9rk4m5lzq6c880000gn/T/ipykernel_17242/2971857110.py:36: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  final_merged_data = final_merged_data.groupby('Suburb').apply(lambda group: group.fillna(group.select_dtypes(include='number').median()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values left in the data.\n",
      "Final merged data saved to ../../data/curated/final_merged_data_sa2.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Load the preprocessed criminal data (aggregated by SA2)\n",
    "aggregated_criminal_data_file = os.path.join(raw_dir, 'rent_data','aggregated_criminal_data_sa2.csv')\n",
    "criminal_data = pd.read_csv(aggregated_criminal_data_file)\n",
    "\n",
    "# Step 2: Load the preprocessed rent data (merged with SA2)\n",
    "rent_data_file = os.path.join(raw_dir, 'rent_data', 'final_rent_data.csv')\n",
    "rent_data = pd.read_csv(rent_data_file)\n",
    "\n",
    "# Step 3: Load the region data (aggregated with distance to Melbourne CBD)\n",
    "region_data_file = os.path.join(raw_dir, 'rent_data', 'final_region_data_with_distance.csv')\n",
    "region_data = pd.read_csv(region_data_file)\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "print(\"Criminal Data: \")\n",
    "print(criminal_data.head())\n",
    "print(\"Rent Data: \")\n",
    "print(rent_data.head())\n",
    "print(\"Region Data: \")\n",
    "print(region_data.head())\n",
    "\n",
    "# Step 4: Merge rent data with preprocessed criminal data on 'SA2'\n",
    "merged_data = pd.merge(rent_data, criminal_data, on='SA2', how='left')\n",
    "\n",
    "# Step 5: Merge the result with region data on 'SA2'\n",
    "final_merged_data = pd.merge(merged_data, region_data, on='SA2', how='left')\n",
    "\n",
    "# Display the first few rows of the final merged dataset to verify the merge\n",
    "print(\"Final Merged Data: \")\n",
    "print(final_merged_data.head())\n",
    "\n",
    "# Step 6: Clean the merged dataset\n",
    "# Drop any rows where the 'SA2' column is blank\n",
    "final_merged_data = final_merged_data.dropna(subset=['SA2'])\n",
    "\n",
    "# Fill missing values within each 'Suburb' (or 'LGA') using the median for each group\n",
    "final_merged_data = final_merged_data.groupby('Suburb').apply(lambda group: group.fillna(group.select_dtypes(include='number').median()))\n",
    "final_merged_data = final_merged_data.dropna()\n",
    "# Rename the 'Suburb' column to 'LGA' for consistency\n",
    "final_merged_data.rename(columns={'Suburb': 'LGA'}, inplace=True)\n",
    "\n",
    "# Step 7: Check if any missing values are left in the dataset\n",
    "missing_values = final_merged_data.isnull().sum().sum()\n",
    "\n",
    "if missing_values == 0:\n",
    "    print(\"No missing values left in the data.\")\n",
    "else:\n",
    "    print(f\"There are still {missing_values} missing values left in the data.\")\n",
    "\n",
    "# Step 8: Drop duplicates based on 'SA2' to ensure unique records per region\n",
    "final_merged_data = final_merged_data.drop_duplicates(subset='SA2', keep='first')\n",
    "\n",
    "# Remove rows where 'SA2' contains the word 'industrial' (case insensitive)\n",
    "final_merged_data = final_merged_data[~final_merged_data['SA2'].str.contains('industrial', case=False, na=False)]\n",
    "\n",
    "\n",
    "\n",
    "output_final_path = os.path.join(curated_dir, 'final_merged_data_sa2.csv')\n",
    "final_merged_data.to_csv(output_final_path, index=False)\n",
    "\n",
    "print(f\"Final merged data saved to {output_final_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
